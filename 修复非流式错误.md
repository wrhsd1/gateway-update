# 修复非流式请求压缩错误问题

## 问题描述

**日期**: 2025-08-07

**错误信息**:
```
[2025-08-07T02:12:46.816Z] Using standard response handling for non-streaming request: gpt-4o-mini
[2025-08-07T02:12:46.817Z] Standard response error for gpt-4o-mini after 1420ms: Error: incorrect header check
    at Zlib.zlibOnError [as onerror] (node:zlib:189:17) {
  errno: -3,
  code: 'Z_DATA_ERROR'
}
```

**症状**: 客户端使用非流式请求（`stream: false`）时，服务端返回 `Z_DATA_ERROR` 错误，这是一个 zlib 压缩库的错误。

## 根本原因

1. **响应压缩处理问题**: 上游 API 返回的响应可能包含 `content-encoding` 头（gzip、deflate、br 等），表示响应体已被压缩。

2. **重复解压缩**: 在 `responseService.ts` 中，用于删除 `content-encoding` 头的代码被注释掉了，导致：
   - Node.js fetch 会根据 `content-encoding` 头自动解压缩响应体
   - 但响应头仍然保留了 `content-encoding`
   - 后续代码再次尝试基于该头解压缩已解压的数据，导致 `Z_DATA_ERROR`

3. **错误处理不足**: `handleNonStreamingMode` 函数缺少对压缩相关错误的处理。

## 解决方案

### 1. 重新启用 content-encoding 头处理

**文件**: `src/handlers/services/responseService.ts`

**修改前**:
```typescript
// Remove headers directly
// const encoding = response.headers.get('content-encoding');
// if (encoding?.includes('br') || getRuntimeKey() == 'node') {
//   response.headers.delete('content-encoding');
// }
response.headers.delete('content-length');
// response.headers.delete('transfer-encoding');
```

**修改后**:
```typescript
// Remove headers directly
const encoding = response.headers.get('content-encoding');
if (encoding?.includes('br') || getRuntimeKey() == 'node') {
  response.headers.delete('content-encoding');
}
response.headers.delete('content-length');
response.headers.delete('transfer-encoding');
```

**添加导入**:
```typescript
import { getRuntimeKey } from 'hono/adapter';
```

### 2. 改进非流式响应的错误处理

**文件**: `src/handlers/streamHandler.ts`

**修改**: 在 `handleNonStreamingMode` 函数中添加完善的错误处理和压缩头处理逻辑

**修改前**:
```typescript
const isJsonParsingRequired = responseTransformer || areSyncHooksAvailable;
const originalResponseBodyJson: Record<string, any> | null =
  isJsonParsingRequired ? await response.json() : null;
let responseBodyJson = originalResponseBodyJson;
```

**修改后**:
```typescript
const isJsonParsingRequired = responseTransformer || areSyncHooksAvailable;
let originalResponseBodyJson: Record<string, any> | null = null;

if (isJsonParsingRequired) {
  try {
    // Clone the response before reading to avoid consuming the stream
    const clonedResponse = response.clone();
    
    // Check if response has content-encoding header that might cause issues
    const contentEncoding = clonedResponse.headers.get('content-encoding');
    if (contentEncoding) {
      // Create a new response without the content-encoding header to avoid decompression issues
      const headers = new Headers(clonedResponse.headers);
      headers.delete('content-encoding');
      headers.delete('content-length');
      
      const responseBody = await clonedResponse.arrayBuffer();
      const newResponse = new Response(responseBody, {
        status: clonedResponse.status,
        statusText: clonedResponse.statusText,
        headers: headers
      });
      originalResponseBodyJson = await newResponse.json();
    } else {
      originalResponseBodyJson = await clonedResponse.json();
    }
  } catch (error) {
    console.error('Error parsing response JSON in non-streaming mode:', error);
    // If JSON parsing fails, try to get text and see if we can work with it
    try {
      const text = await response.clone().text();
      console.log(`Response text: ${text.substring(0, 200)}...`);
      // Try to parse as JSON manually
      originalResponseBodyJson = JSON.parse(text);
    } catch (textError) {
      console.error('Failed to parse response as JSON or text:', textError);
      // Return the original response if we can't parse it
      return {
        response: new Response(response.body, response),
        json: null,
        originalResponseBodyJson: null,
      };
    }
  }
}

let responseBodyJson = originalResponseBodyJson;
```

### 3. 增强日志和错误追踪

**文件**: `src/handlers/responseHandlers.ts`

**添加日志**:
```typescript
const nonStreamingResponse = await handleNonStreamingMode(
  response,
  responseTransformerFunction,
  strictOpenAiCompliance,
  gatewayRequestUrl,
  gatewayRequest,
  areSyncHooksAvailable
).catch(error => {
  const executionTime = Date.now() - (context?.get('requestStartTime') || Date.now());
  console.error(`[${beijingTime}] Standard response error for ${gatewayRequest.model} after ${executionTime}ms:`, error);
  throw error;
});

console.log(`[${beijingTime}] Using standard response handling for non-streaming request: ${gatewayRequest.model}`);
```

### 4. 改进 .gitignore

添加了以下忽略规则：
- 测试文件: `test_*.js`, `test-*.js`
- 日志文件: `public/log.log`, `public/*.log`
- 临时文件: `*.tmp`, `*.temp`, `*.bak`
- 敏感文件: `*.key`, `*.pem`, `secrets/`

## 技术细节

### 为什么会出现 Z_DATA_ERROR

1. **自动解压缩**: Node.js 的 fetch 实现会根据 `content-encoding` 头自动解压缩响应体
2. **头信息保留**: 但解压后的响应对象仍然保留原始的 `content-encoding` 头
3. **二次解压**: 当代码尝试再次基于这个头处理数据时，会尝试解压已经解压的数据
4. **错误触发**: zlib 库检测到数据格式不正确，抛出 `Z_DATA_ERROR`

### 解决方案的工作原理

1. **提前删除压缩头**: 在 `responseService.ts` 中，检测到压缩头后立即删除，避免下游代码误判
2. **安全的 JSON 解析**: 在解析 JSON 前检查压缩头，如果存在则创建新的响应对象（不带压缩头）
3. **多层错误处理**: 添加 try-catch 块，即使解析失败也能降级处理
4. **详细日志**: 记录错误信息和执行时间，便于调试

## 测试验证

### 测试场景

1. **非流式请求**: `stream: false`
2. **带压缩头的响应**: 上游 API 返回 `content-encoding: gzip/deflate/br`
3. **大型响应**: 测试大数据量的压缩响应

### 预期结果

- ✅ 请求成功完成，无 `Z_DATA_ERROR` 错误
- ✅ 响应正确解析为 JSON
- ✅ 日志显示 "Using standard response handling for non-streaming request"

## 相关文件

- `src/handlers/services/responseService.ts`
- `src/handlers/streamHandler.ts`
- `src/handlers/responseHandlers.ts`
- `.gitignore`

## Git Hooks 说明

项目使用 husky 管理 Git hooks：

- **pre-commit**: 检查代码格式，自动格式化
- **pre-push**: 运行构建和测试 (`npm run build && node start-test.js`)

### 跳过 hooks

如果需要快速推送代码，可以使用：
```bash
git push --no-verify
```

或暂时禁用 husky：
```bash
export HUSKY=0
git push origin main
```

## 总结

此次修复解决了非流式请求时的压缩错误问题，主要通过：
1. 正确处理 `content-encoding` 头
2. 增强错误处理和降级机制
3. 添加详细日志用于监控和调试

修复后，系统能够正确处理来自上游 API 的压缩响应，避免重复解压缩导致的错误。
